<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Robust e-NeRF directly and robustly reconstructs NeRFs from moving event cameras under various real-world conditions, especially from sparse and noisy events generated under non-uniform motion.">
  <meta name="keywords" content="Robust e-NeRF, NeRF, Event Camera">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion</title>

  <!-- OpenGraph -->
  <meta property="og:site_name" content="Weng Fei Low 刘永辉" />
  <meta property="og:type" content="website" />
  <meta property="og:title" content="Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion" />
  <meta property="og:url" content="https://wengflow.github.io/robust-e-nerf" />
  <meta property="og:description" content="Robust e-NeRF directly and robustly reconstructs NeRFs from moving event cameras under various real-world conditions, especially from sparse and noisy events generated under non-uniform motion." />
  <!-- <meta property="og:image" content="{%- if page.og_image -%}{{ page.og_image }}{%- else -%}{{ site.og_image }}{%- endif -%}" /> -->
  <meta property="og:locale" content="en" />

  <!-- Twitter card -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion" />
  <meta name="twitter:description" content="Robust e-NeRF directly and robustly reconstructs NeRFs from moving event cameras under various real-world conditions, especially from sparse and noisy events generated under non-uniform motion." />
  <!-- <meta name="twitter:image" content="{%- if page.og_image -%}{{ page.og_image }}{%- else -%}{{ site.og_image }}{%- endif -%}" /> -->
  <meta name="twitter:site" content="@wengflow" />
  <meta name="twitter:creator" content="@wengflow" />

  <script type="application/ld+json">
    {
      "author":
      {
        "@type": "Person",
        "name": "Weng Fei Low"
      },
      "url": "https://wengflow.github.io/robust-e-nerf",
      "@type": "WebSite",
      "description": "Robust e-NeRF directly and robustly reconstructs NeRFs from moving event cameras under various real-world conditions, especially from sparse and noisy events generated under non-uniform motion.",
      "headline": "Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion",
      "name": "Weng Fei Low",
      "@context": "https://schema.org"
    }
  </script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4WDKH4YC37"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-4WDKH4YC37');
  </script>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

    <!-- Generated from realfavicongenerator.net and then Liquid tags are inserted-->
    <link rel="apple-touch-icon" sizes="180x180" href="https://wengflow.github.io/assets/img/icon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://wengflow.github.io/assets/img/icon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://wengflow.github.io/assets/img/icon/favicon-16x16.png">
    <link rel="manifest" href="https://wengflow.github.io/assets/img/icon/site.webmanifest">
    <link rel="shortcut icon" href="https://wengflow.github.io/assets/img/icon/favicon.ico">
    <meta name="msapplication-TileColor" content="#ffc40d">
    <meta name="msapplication-config" content="https://wengflow.github.io/assets/img/icon/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        macros: { // Reference: https://github.com/mathjax/MathJax/issues/1219
          bm: ["{\\boldsymbol #1}", 1],
        }
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Robust <i>e-</i>NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion</h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://wengflow.github.io/">Weng Fei Low</a>,</span>
            <span class="author-block">
              <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">National University of Singapore</span>
          </div>

          <div class="is-size-5 publication-authors"><b>International Conference on Computer Vision (ICCV) 2023</b></div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2309.08596"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Poster Link. -->
              <span class="link-block">
                <a href="./static/pdfs/poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="./#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/wengflow/robust-e-nerf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Simulator Link. -->
              <span class="link-block">
                <a href="https://github.com/wengflow/rpg_esim"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-codepen"></i>
                  </span>
                  <span>Simulator</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/wengflow/robust-e-nerf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop has-text-centered">

    <div class="columns is-centered">
      <div class="column is-three-fifths p-4 has-text-centered">
        <video id="comparison" autoplay muted loop playsinline>
          <source src="./static/videos/comparison.mp4"
                  type="video/mp4">
        </video>
      </div>
      <div class="column is-two-fifths p-4 has-text-centered" style="border-left: 2px solid #f5f5f5;">
        <video id="results" autoplay muted loop playsinline>
          <source src="./static/videos/results (square).mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>
    <!-- <h2 class="subtitle has-text-centered">
      <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
      free-viewpoint
      portraits.
    </h2> -->

    <!-- <div class="hero-body">
      <video id="teaser1" autoplay muted loop playsinline width="70%">
        <source src="./static/videos/comparison.mp4"
                type="video/mp4">
      </video>
      <br><br>
      <hr style="width:70%; margin: auto;">
      <br>
      <video id="teaser2" autoplay muted loop playsinline width="70%">
        <source src="./static/videos/results.mp4"
                type="video/mp4">
      </video>
    </div>
     -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Event cameras offer many advantages over standard cameras due to their distinctive principle 
            of operation: low power, low latency, high temporal resolution and high dynamic range. 
            Nonetheless, the success of many downstream visual applications also hinges on an efficient 
            and effective scene representation, where Neural Radiance Field (NeRF) is seen as the leading candidate.
          </p>

          <p>
            Such promise and potential of event cameras and NeRF inspired recent works to investigate on 
            the reconstruction of NeRF from moving event cameras. However, these works are mainly limited 
            in terms of the dependence on dense and low-noise event streams, as well as generalization to 
            arbitrary contrast threshold values and camera speed profiles.
          </p>

          <p>
            In this work, we propose Robust <i>e-</i>NeRF, a novel method to directly and robustly reconstruct 
            NeRFs from moving event cameras under various real-world conditions, especially from sparse and 
            noisy events generated under non-uniform motion. It consists of two key components: a realistic 
            event generation model that accounts for various intrinsic parameters (<i>e.g.</i> time-independent, 
            asymmetric threshold and refractory period) and non-idealities (<i>e.g.</i> pixel-to-pixel threshold 
            variation), as well as a complementary pair of normalized reconstruction losses that can 
            effectively generalize to arbitrary speed profiles and intrinsic parameter values without such 
            prior knowledge. Experiments on real and novel realistically simulated sequences verify our effectiveness. Our 
            code, synthetic dataset and improved event simulator are public.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->

  </div>
</section>

<hr>

<section class="section">
  <div class="container is-max-desktop">

    <h2 class="title is-3">Event Generation Model</h2>
    <br>
    <div class="content is-centered has-text-justified">
      <figure class="image">
        <img src="./static/images/egm.png" style="max-width: 400px">
      </figure>

      <p>
        An event $\bm{e}$ of polarity $p$ is generated at timestamp $t_{\mathit{curr}}$ when the difference in log-radiance 
        $\log L$ at a pixel $\bm{u}$, measured with respect to a reference $\log L$ at timestamp $t_{\mathit{ref}}$, has the 
        same sign as $p$ and a magnitude that equals to the contrast threshold associated to polarity $p$, $C_p$. Red, 
        downwards and blue, upwards arrows represent events of polarities $-1$ and $+1$, respectively, and each right-angled 
        dashed line represents the measured change in $\log L$.
      </p>
      
      <p>
        After an event is generated, the pixel will be temporarily deactivated for an amount of time given by the refractory 
        period $\tau$, as shaded in the figure. Thus, $t_{\mathit{ref}}$ is simply the sum of the previous event timestamp 
        $t_{\mathit{prev}}$ and $\tau$.
      </p>
    </div>

  </div>
  <br><br>
<!-- </section>

<section class="section"> -->
  <div class="container is-max-desktop">

    <h2 class="title is-3">Training Pipeline</h2>
    <br>
    <div class="content is-centered has-text-justified">
      <figure class="image">
        <img src="./static/images/overview.png">
      </figure>

      <p>
        For each event $\bm{e}$ in the batch $\mathcal{E}_{\mathit{batch}}$ sampled randomly from the event stream, we first 
        derive the reference timestamp $t_{\mathit{ref}}$, given the refractory period $\tau$, and sample a timestamp 
        $t_{\mathit{sam}}$ between $t_{\mathit{ref}}$ and $t_{\mathit{curr}}$. Next, we interpolate the given constant-rate 
        camera poses at $t_{\mathit{ref}}, t_{\mathit{sam}}$ and $t_{\mathit{curr}}$ using LERP for position and SLERP for orientation. 
      </p>
        
      <p>
        Given these pose estimates $\hat{T}$, we then perform volume rendering on the back-projected rays from pixel $\bm{u}$ 
        with the NeRF $F_\Theta$. This is done to infer the predicted radiance $\hat{L}$, and thus log-radiance $\log{\hat{L}}$, 
        of pixel $\bm{u}$ at $t_{\mathit{ref}}, t_{\mathit{sam}}$ and $t_{\mathit{curr}}$. For brevity, we denote $\hat{L} 
        (t) = \hat{L} (\bm{u}, t)$. 
      </p>
        
      <p>
        These $\log{\hat{L}}$ are ultimately used to derive the 
        predicted log-radiance difference $\Delta \log \hat{L}$ and gradient $\frac{\partial}{\partial t} \log \hat{L} 
        (t_{\mathit{sam}})$ for the computation of the reconstruction loss: threshold-normalized difference loss 
        $\ell_{\mathit{diff}}$ and smoothness loss: target-normalized gradient loss $\ell_{\mathit{grad}}$, given the observed 
        log-radiance difference $\Delta \log L$ and gradient $\frac{\partial}{\partial t} \log L (t_{\mathit{sam}})$ 
        approximation from the event $\bm{e}$, respectively.
      </p>
    </div>

  </div>
<!-- </section> -->

  <br><br>
  <div class="container is-max-desktop">

    <h2 class="title is-3">Results</h2>

    <h3 class="title is-4">Overall</h3>
    <br>
    <div class="content is-centered has-text-justified">
      <figure class="image">
        <img src="./static/images/qualitative.png">
      </figure>
    </div>

    <br>
    <h3 class="title is-4">Dense and Noise-Free Events under Uniform Motion</h3>
    <br>
    <div class="content is-centered has-text-centered">

      <div id="results-carousel" class="carousel results-carousel">
        
        <div class="item">
          <figure class="image">
            <img src="./static/images/easy/chair.png">
          </figure>
        </div>
        <div class="item">
          <figure class="image">
            <img src="./static/images/easy/drums.png">
          </figure>
        </div>
        <div class="item">
          <figure class="image">
            <img src="./static/images/easy/ficus.png">
          </figure>
        </div>
        <div class="item">
          <figure class="image">
            <img src="./static/images/easy/hotdog.png">
          </figure>
        </div>
        <div class="item">
          <figure class="image">
            <img src="./static/images/easy/lego.png">
          </figure>
        </div>
        <div class="item">
          <figure class="image">
            <img src="./static/images/easy/materials.png">
          </figure>
        </div>
        <div class="item">
          <figure class="image">
            <img src="./static/images/easy/mic.png">
          </figure>
        </div>

      </div>

      <br>
      <p>
        Robust <i>e-</i>NeRF also better reconstructs fine details and preserves color accuracy, especially at the background.
      </p>
    </div>

    <br>
    <h3 class="title is-4">Robustness to Temporal Event Sparsity</h3>
    <br>
    <div class="content is-centered has-text-justified">
      <figure class="image">
        <img src="./static/images/data_efficiency.png">
      </figure>

      <p>
        Robust <i>e-</i>NeRF demonstrates astonishing robustness under severely sparse event streams, which suggests its high 
        data efficiency. Notably, reasonable accuracy can still be achieved with $\tau = 1000 \mathit{ms}$, where each pixel 
        can only generate at most 4 events throughout the $4000\mathit{ms}$ sequence and the event stream is around $200 \times$ 
        sparser than that of $\tau = 0 \mathit{ms}$.
      </p>
    </div>

  </div>
</section>

<hr>

<section class="section" id="BibTeX">
  <!-- Acknowledgements. -->
  <div class="container is-max-desktop content has-text-justified">
    <h2 class="title is-3">Acknowledgements</h2>
    <p>
      This research is supported by the National Research Foundation, Singapore under its 
      AI Singapore Programme (AISG Award No: AISG2-RP-2021024), and the Tier 2 grant
      MOE-T2EP20120-0011 from the Singapore Ministry of Education.
    </p>
  </div>
  <!--/ Acknowledgements. -->
  <br>

  <!-- BibTeX. -->
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{low2023_robust-e-nerf,
  title = {Robust e-NeRF: NeRF from Sparse & Noisy Events under Non-Uniform Motion},
  author = {Low, Weng Fei and Lee, Gim Hee},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year = {2023}
}</code></pre>
  </div>
  <!--/ BibTeX. -->
  <br>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            <br>The template for this website is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.</p>
          </p>
        </div>
      </div>
    </div>

  </div>
</footer>

</body>
</html>
